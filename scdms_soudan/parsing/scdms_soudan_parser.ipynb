{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from construct import *\n",
    "\n",
    "format_word = Struct(\n",
    "    \"daq_major\" / Byte,\n",
    "    \"daq_minor\" / Byte,\n",
    "    \"data_format_major\" / Byte,\n",
    "    \"data_format_minor\" / Byte\n",
    ")\n",
    "\n",
    "two_word_file_header = Struct(\n",
    "    \"endian_indicator\" / Int32ul,\n",
    "    \"data_format\" / format_word\n",
    ")\n",
    "\n",
    "detector_hdr = Struct(\n",
    "    \"header_number\" / Int32ul,\n",
    "    \"config_record_len\" / Int32ul,\n",
    "    \"repeat_value\" / Computed(\n",
    "        lambda this: (this.config_record_len // 72) + (this.config_record_len // 144)\n",
    "    )\n",
    ")\n",
    "\n",
    "charge_config_header = Struct(\n",
    "    \"charge_config_len\" / Int32ul,\n",
    "    \"detector_code\" / Int32sl,\n",
    "    \"tower_number\" / Int32sl,\n",
    "    \"channel_post_amp\" / Int32sl,\n",
    "    \"channel_bias\" / Int32sl,\n",
    "    \"rtf_offset\" / Int32sl,\n",
    "    \"delta_t\" / Int32sl,\n",
    "    \"trigger_time\" / Int32sl,\n",
    "    \"trace_len\" / Int32sl\n",
    ")\n",
    "\n",
    "phonon_config_header = Struct(\n",
    "    \"phonon_config_len\" / Int32ul,\n",
    "    \"detector_code\" / Int32sl,\n",
    "    \"tower_number\" / Int32sl,\n",
    "    \"post_amp_gain\" / Int32sl,\n",
    "    \"qet_bias\" / Int32sl,\n",
    "    \"squid_bias\" / Int32sl,\n",
    "    \"squid_lockpoint\" / Int32sl,\n",
    "    \"rtf_offset\" / Int32sl,\n",
    "    \"variable_gain\" / Int32sl,\n",
    "    \"delta_t\" / Int32sl,\n",
    "    \"trigger_time\" / Int32sl,\n",
    "    \"trace_len\" / Int32sl\n",
    ")\n",
    "\n",
    "header_list = Struct(\n",
    "    \"header_number\" / Int32ul,\n",
    "    \"charge_config\" / If(\n",
    "        lambda this: this.header_number == 0x10002,\n",
    "        charge_config_header\n",
    "    ),\n",
    "    \"phonon_config\" / If(\n",
    "        lambda this: this.header_number == 0x10001,\n",
    "        phonon_config_header\n",
    "    )\n",
    ")\n",
    "\n",
    "event_header = Struct(\n",
    "    \"event_header_word\" / Int32ul,\n",
    "    \"event_size\" / Int32ul,\n",
    "    \"event_identifier\" / Computed(\n",
    "        lambda this: (this.event_header_word >> 16) & 0xFFFF\n",
    "    ),\n",
    "    # 0x0: Raw, 0x1: Processed, 0x2: Monte Carlo\n",
    "    \"event_class\" / Computed(\n",
    "        lambda this: (this.event_header_word >> 8) & 0xF\n",
    "    ),\n",
    "    # 0x0: Per Trigger, 0x1: Occasional, 0x2: Begin File Series, 0x3: Begin File\n",
    "    # 0x4: End File, 0x5: End File Series, 0x6: Per Trigger w/ Detectors that Cross Threshold\n",
    "    \"event_category\" / Computed(\n",
    "        lambda this: (this.event_header_word >> 12) & 0xF\n",
    "    ),\n",
    "    # 0x0: Wimp Search, 0x1: 60Co Calibration, 0x2: 60Co Low Energy Calibration,\n",
    "    # 0x3: Neutron Calibration, 0x4: Random Triggers, 0x5: Pulse Triggers\n",
    "    # 0x6: Test, 0x7: Data Monitering Event, 0x8: 137Cs Calibration\n",
    "    \"event_type\" / Computed(\n",
    "        lambda this: (this.event_header_word & 0xFF)\n",
    "    )\n",
    ")\n",
    "\n",
    "administrative_record = Struct(\n",
    "    \"admin_header\" / Int32ul,\n",
    "    \"admin_len\" / Int32ul,\n",
    "    \"series_number_1\" / Int32ul,\n",
    "    \"series_number_2\" / Int32ul,\n",
    "    \"event_number_in_series\" / Int32ul,\n",
    "    \"seconds_from_epoch\" / Int32ul,\n",
    "    # Epoch defined as Jan 1st 1904 for SUF (MAC Artifact)\n",
    "    # Epoch defined as Jan 1st 1970 for Soudan\n",
    "    \"time_from_last_event\" / Int32ul,\n",
    "    \"live_time_from_last_event\" / Int32ul\n",
    ")\n",
    "\n",
    "trace_record = Struct(\n",
    "    \"trace_header\" / Int32ul,\n",
    "    \"trace_len\" / Int32ul,\n",
    "    \"trace_bookkeeping_header\" / Int32ul,\n",
    "    \"bookkeeping_len\" / Int32ul,\n",
    "    \"digitizer_base_address\" / Int32ul,\n",
    "    \"digitizer_channel\" / Int32ul,\n",
    "    \"detector_code\" / Int32ul,\n",
    "    \"timebase_header\" / Int32ul,\n",
    "    \"timebase_len\" / Int32ul,\n",
    "    \"t0_in_ns\" / Int32ul,\n",
    "    \"delta_t_ns\" / Int32ul,\n",
    "    \"num_of_points\" / Int32ul,\n",
    "    \"second_trace_header\" / Int32ul,\n",
    "    \"num_samples\" / Int32ul\n",
    "    # Should be a power of two (1024, 2048, etc)\n",
    ")\n",
    "\n",
    "data_sample = Struct(\n",
    "    \"data_selection\" / Int32ul,\n",
    "    \"sample_a\" / Computed(\n",
    "        lambda this: (this.data_selection >> 16) & 0xFFFF\n",
    "    ),\n",
    "    \"sample_b\" / Computed(\n",
    "        lambda this: (this.data_selection & 0xFFFF)\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "trace_data = Struct(\n",
    "    \"trace_rcrds\" / trace_record,\n",
    "    \"sample_data\" / Array(\n",
    "        this.trace_rcrds.num_samples // 2,\n",
    "        data_sample\n",
    "    )\n",
    ")\n",
    "\n",
    "soudan_history_buffer = Struct(\n",
    "    \"history_buffer_header\" / Int32ul,\n",
    "    \"history_buffer_len\" / Int32ul,\n",
    "    \"num_time_nvt\" / Int32ul,\n",
    "    \"time_nvt\" / Array(\n",
    "        this.num_time_nvt,\n",
    "        Int32ul\n",
    "    ),\n",
    "    \"num_veto_mask_words\" / Int32ul,\n",
    "    \"time_n_minus_veto_mask\" / Array(\n",
    "        this.num_time_nvt * this.num_veto_mask_words,\n",
    "        Int32ul\n",
    "    ),\n",
    "    \"num_trigger_times\" / Int32ul,\n",
    "    \"trigger_times\" / Array(\n",
    "        this.num_trigger_times,\n",
    "        Int32ul\n",
    "    ),\n",
    "    \"num_trigger_mask_words\" / Int32ul,\n",
    "    \"trig_times_minus_trig_mask\" / Array(\n",
    "        this.num_trigger_times * this.num_trigger_mask_words,\n",
    "        Int32ul\n",
    "    )\n",
    ")\n",
    "\n",
    "trigger_record = Struct(\n",
    "    \"trigger_header\" / Int32ul,\n",
    "    \"trigger_len\" / Int32ul,\n",
    "    \"trigger_time\" / Int32ul,\n",
    "    \"individual_trigger_masks\" / Array(\n",
    "        6,\n",
    "        Int32ul\n",
    "    )\n",
    ")\n",
    "\n",
    "tlb_trigger_mask_record = Struct(\n",
    "    \"tlb_mask_header\" / Int32ul,\n",
    "    \"tlb_len\" / Int32ul,\n",
    "    \"tower_mask\" / Array(\n",
    "        6,\n",
    "        Int32ul\n",
    "    )\n",
    ")\n",
    "\n",
    "gps_data = Struct(\n",
    "    \"tlb_mask_header\" / Int32ul,\n",
    "    \"length\" / Int32ul,\n",
    "    \"gps_year_day\" / If(\n",
    "        this.length > 0,\n",
    "        Int32ul\n",
    "    ),\n",
    "    \"gps_status_hour_minute_second\" / If(\n",
    "        this.length > 0,\n",
    "        Int32ul\n",
    "    ),\n",
    "    \"gps_microsecs_from_gps_second\" / If(\n",
    "        this.length > 0,\n",
    "        Int32ul\n",
    "    )\n",
    ")\n",
    "\n",
    "detector_trigger_threshold_data = Struct(\n",
    "    \"threshold_header\" / Int32ul,\n",
    "    \"len_to_next_header\" / Int32ul,\n",
    "    \"minimum_voltage_level\" / Int32ul,\n",
    "    \"maximum_voltage_level\" / Int32ul,\n",
    "    \"dynamic_range\" / Int32ul,\n",
    "    \"tower_number\" / Int32ul,\n",
    "    \"detector_codes\" / Array(\n",
    "        6,\n",
    "        Int32ul\n",
    "    ),\n",
    "    \"operations_codes\" / Array(\n",
    "        9,\n",
    "        Int32ul\n",
    "    ),\n",
    "    \"adc_values\" / Array(\n",
    "        54,\n",
    "        Int32ul\n",
    "    )\n",
    ")\n",
    "\n",
    "detector_trigger_rates = Struct(\n",
    "    \"detector_trigger_header\" / Int32ul,\n",
    "    \"len_to_next_header\" / Int32ul,\n",
    "    \"clocking_interval\" / Int32ul,\n",
    "    \"tower_number\" / Int32ul,\n",
    "    \"detector_codes\" / Array(\n",
    "        6,\n",
    "        Int32ul\n",
    "    ),\n",
    "    \"j_codes\" / Array(\n",
    "        5,\n",
    "        Int32ul\n",
    "    ),\n",
    "    \"counter_values\" / Array(\n",
    "        30,\n",
    "        Int32ul\n",
    "    )\n",
    ")\n",
    "\n",
    "veto_trigger_rates = Struct(\n",
    "    \"veto_trigger_header\" / Int32ul,\n",
    "    \"len_to_next_header\" / Int32ul,\n",
    "    \"clocking_interval\" / Int32ul,\n",
    "    \"num_entries\" / Int32ul,\n",
    "    \"detector_code\" / Array(\n",
    "        this.num_entries,\n",
    "        Int32ul\n",
    "    ),\n",
    "    \"counter_value_det_code\" / Array(\n",
    "        this.num_entries,\n",
    "        Int32ul\n",
    "    )\n",
    ")\n",
    "\n",
    "# Trying to generalize the logical records type\n",
    "logical_records_unused = Struct(\n",
    "    \"event_hdr\" / event_header,\n",
    "    \"next_section\" / Array(\n",
    "        6,\n",
    "        Struct(\n",
    "            \"next_header\" / Peek(Int32ul),\n",
    "            \"section\" / Switch(\n",
    "                this.next_header,\n",
    "                {\n",
    "                    0x00000002: administrative_record,\n",
    "                    0x00000011: trace_data,\n",
    "                    0x00000021: soudan_history_buffer,\n",
    "                    0x00000060: gps_data,\n",
    "                    0x00000080: trigger_record,\n",
    "                    0x00000081: tlb_trigger_mask_record,\n",
    "                    0x00000022: detector_trigger_rates,\n",
    "                    0x00000031: veto_trigger_rates\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "logical_records = Struct(\n",
    "    \"event_hdr\" / event_header,\n",
    "    \"admin_rcrd\" / administrative_record,\n",
    "    \"trigger_rcrd\" / trigger_record,\n",
    "    \"tlb_trig_mask_rcrd\" / tlb_trigger_mask_record,\n",
    "    \"gps_data\" / gps_data,\n",
    "    \"trace_data\" / Array(\n",
    "        this._root.detector_hdr.repeat_value,\n",
    "        trace_data\n",
    "    ),\n",
    "    \"soudan_buffer\" / soudan_history_buffer#,\n",
    "    #\"detector_threshold_data\" / detector_trigger_threshold_data,\n",
    "    #\"detector_trig_rates\" / detector_trigger_rates,\n",
    "    #\"veto_trig_rates\" / veto_trigger_rates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soudan = Struct(\n",
    "    \"file_hdr\" / two_word_file_header,\n",
    "    \"detector_hdr\" / detector_hdr,\n",
    "    \"hdrs\" / Array(\n",
    "        this._root.detector_hdr.repeat_value,\n",
    "        header_list\n",
    "    ),\n",
    "    \"logical_rcrds\" / GreedyRange(logical_records)\n",
    ")\n",
    "\n",
    "# Parsing a smaller number of logical_rcrds\n",
    "test = Struct(\n",
    "    \"file_hdr\" / two_word_file_header,\n",
    "    \"detector_hdr\" / detector_hdr,\n",
    "    \"hdrs\" / Array(\n",
    "        this._root.detector_hdr.repeat_value,\n",
    "        header_list\n",
    "    ),\n",
    "    \"logical_rcrds\" / Array(\n",
    "        2,\n",
    "        logical_records\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples in event 0: 2048\n",
      "Data samples in event 1: 2048\n",
      "Data samples in event 2: 2048\n",
      "Data samples in event 3: 2048\n",
      "Data samples in event 4: 2048\n",
      "Data samples in event 5: 2048\n",
      "Data samples in event 6: 2048\n",
      "Data samples in event 7: 2048\n",
      "Data samples in event 8: 2048\n",
      "Data samples in event 9: 2048\n",
      "Data samples in event 10: 2048\n",
      "Data samples in event 11: 2048\n",
      "Data samples in event 12: 2048\n",
      "Data samples in event 13: 2048\n",
      "Data samples in event 14: 2048\n",
      "Data samples in event 15: 2048\n",
      "Data samples in event 16: 2048\n",
      "Data samples in event 17: 2048\n",
      "Data samples in event 18: 2048\n",
      "Data samples in event 19: 2048\n",
      "Data samples in event 20: 2048\n",
      "Data samples in event 21: 2048\n",
      "Data samples in event 22: 2048\n",
      "Data samples in event 23: 2048\n",
      "Data samples in event 24: 2048\n",
      "Data samples in event 25: 2048\n",
      "Data samples in event 26: 2048\n",
      "Data samples in event 27: 2048\n",
      "Data samples in event 28: 2048\n",
      "Data samples in event 29: 2048\n",
      "Data samples in event 30: 2048\n",
      "Data samples in event 31: 2048\n",
      "Data samples in event 32: 2048\n",
      "Data samples in event 33: 2048\n",
      "Data samples in event 34: 2048\n",
      "Data samples in event 35: 2048\n",
      "Data samples in event 36: 2048\n",
      "Data samples in event 37: 2048\n",
      "Data samples in event 38: 2048\n",
      "Data samples in event 39: 2048\n",
      "Data samples in event 40: 2048\n",
      "Data samples in event 41: 2048\n",
      "Data samples in event 42: 2048\n",
      "Data samples in event 43: 2048\n",
      "Data samples in event 44: 2048\n",
      "Data samples in event 45: 2048\n",
      "Data samples in event 46: 2048\n",
      "Data samples in event 47: 2048\n",
      "Data samples in event 48: 2048\n",
      "Data samples in event 49: 2048\n",
      "Data samples in event 50: 2048\n",
      "Data samples in event 51: 2048\n",
      "Data samples in event 52: 2048\n",
      "Data samples in event 53: 2048\n",
      "Data samples in event 54: 2048\n",
      "Data samples in event 55: 2048\n",
      "Data samples in event 56: 2048\n",
      "Data samples in event 57: 2048\n",
      "Data samples in event 58: 2048\n",
      "Data samples in event 59: 2048\n",
      "Data samples in event 60: 2048\n",
      "Data samples in event 61: 2048\n",
      "Data samples in event 62: 2048\n",
      "Data samples in event 63: 2048\n",
      "Data samples in event 64: 2048\n",
      "Data samples in event 65: 2048\n",
      "Data samples in event 66: 2048\n",
      "Data samples in event 67: 2048\n",
      "Data samples in event 68: 2048\n",
      "Data samples in event 69: 2048\n",
      "Data samples in event 70: 2048\n",
      "Data samples in event 71: 2048\n",
      "Data samples in event 72: 2048\n",
      "Data samples in event 73: 2048\n",
      "Data samples in event 74: 2048\n",
      "Data samples in event 75: 2048\n",
      "Data samples in event 76: 2048\n",
      "Data samples in event 77: 2048\n",
      "Data samples in event 78: 2048\n",
      "Data samples in event 79: 2048\n",
      "Data samples in event 80: 2048\n",
      "Data samples in event 81: 2048\n",
      "Data samples in event 82: 2048\n",
      "Data samples in event 83: 2048\n",
      "Data samples in event 84: 2048\n",
      "Data samples in event 85: 2048\n",
      "Data samples in event 86: 2048\n",
      "Data samples in event 87: 2048\n",
      "Data samples in event 88: 2048\n",
      "Data samples in event 89: 2048\n",
      "Data samples in event 90: 2048\n",
      "Data samples in event 91: 2048\n",
      "Data samples in event 92: 2048\n",
      "Data samples in event 93: 2048\n",
      "Data samples in event 94: 2048\n",
      "Data samples in event 95: 2048\n",
      "Data samples in event 96: 2048\n",
      "Data samples in event 97: 2048\n",
      "Data samples in event 98: 2048\n",
      "Data samples in event 99: 2048\n",
      "Data samples in event 100: 2048\n",
      "Data samples in event 101: 2048\n",
      "Data samples in event 102: 2048\n",
      "Data samples in event 103: 2048\n",
      "Data samples in event 104: 2048\n",
      "Data samples in event 105: 2048\n",
      "Data samples in event 106: 2048\n",
      "Data samples in event 107: 2048\n",
      "Data samples in event 108: 2048\n",
      "Data samples in event 109: 2048\n",
      "Data samples in event 110: 2048\n",
      "Data samples in event 111: 2048\n",
      "Data samples in event 112: 2048\n",
      "Data samples in event 113: 2048\n",
      "Data samples in event 114: 2048\n",
      "Data samples in event 115: 2048\n",
      "Data samples in event 116: 2048\n",
      "Data samples in event 117: 2048\n",
      "Data samples in event 118: 2048\n",
      "Data samples in event 119: 2048\n",
      "Data samples in event 120: 2048\n",
      "Data samples in event 121: 2048\n",
      "Data samples in event 122: 2048\n",
      "Data samples in event 123: 2048\n",
      "Data samples in event 124: 2048\n",
      "Data samples in event 125: 2048\n",
      "Data samples in event 126: 2048\n",
      "Data samples in event 127: 2048\n",
      "Data samples in event 128: 2048\n",
      "Data samples in event 129: 2048\n",
      "Data samples in event 130: 2048\n",
      "Data samples in event 131: 2048\n",
      "Data samples in event 132: 2048\n",
      "Data samples in event 133: 2048\n",
      "Data samples in event 134: 2048\n",
      "Data samples in event 135: 2048\n",
      "Data samples in event 136: 2048\n",
      "Data samples in event 137: 2048\n",
      "Data samples in event 138: 2048\n",
      "Data samples in event 139: 2048\n",
      "Data samples in event 140: 2048\n",
      "Data samples in event 141: 2048\n",
      "Data samples in event 142: 2048\n",
      "Data samples in event 143: 2048\n",
      "Data samples in event 144: 2048\n",
      "Data samples in event 145: 2048\n",
      "Data samples in event 146: 2048\n",
      "Data samples in event 147: 2048\n",
      "Data samples in event 148: 2048\n",
      "Data samples in event 149: 2048\n",
      "Data samples in event 150: 2048\n",
      "Data samples in event 151: 2048\n",
      "Data samples in event 152: 2048\n",
      "Data samples in event 153: 2048\n",
      "Data samples in event 154: 2048\n",
      "Data samples in event 155: 2048\n",
      "Data samples in event 156: 2048\n",
      "Data samples in event 157: 2048\n",
      "Data samples in event 158: 2048\n",
      "Data samples in event 159: 2048\n",
      "Data samples in event 160: 2048\n",
      "Data samples in event 161: 2048\n",
      "Data samples in event 162: 2048\n",
      "Data samples in event 163: 2048\n",
      "Data samples in event 164: 2048\n",
      "Data samples in event 165: 2048\n",
      "Data samples in event 166: 2048\n",
      "Data samples in event 167: 2048\n",
      "Data samples in event 168: 2048\n",
      "Data samples in event 169: 2048\n",
      "Data samples in event 170: 2048\n",
      "Data samples in event 171: 2048\n",
      "Data samples in event 172: 2048\n",
      "Data samples in event 173: 2048\n",
      "Data samples in event 174: 2048\n",
      "Data samples in event 175: 2048\n",
      "Data samples in event 176: 2048\n",
      "Data samples in event 177: 2048\n",
      "Data samples in event 178: 2048\n",
      "Data samples in event 179: 2048\n",
      "Data samples in event 180: 2048\n",
      "Data samples in event 181: 2048\n",
      "Data samples in event 182: 2048\n",
      "Data samples in event 183: 2048\n",
      "Data samples in event 184: 2048\n",
      "Data samples in event 185: 2048\n",
      "Data samples in event 186: 2048\n",
      "Data samples in event 187: 2048\n",
      "Data samples in event 188: 2048\n",
      "Data samples in event 189: 2048\n",
      "Data samples in event 190: 2048\n",
      "Data samples in event 191: 2048\n",
      "Data samples in event 192: 2048\n",
      "Number of events parsed: 193\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "def parse_file(input_path, output_path):\n",
    "    with open(input_path, 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        parsed_data = test.parse(raw_data)\n",
    "\n",
    "    with h5py.File(output_path, 'w') as f:\n",
    "        \n",
    "        # Initializing header groups to fill with datasets\n",
    "        file_hdr_grp = f.create_group('file_hdr')\n",
    "        detector_hdr_grp = f.create_group('detector_hdr')\n",
    "\n",
    "        # Initializing arrays for the header information\n",
    "        file_hdr_word_list = []\n",
    "        det_hdr_list = []\n",
    "\n",
    "        # file_hdr and detector_hdr contain no arrays\n",
    "        for file_hdr_type in parsed_data.file_hdr:\n",
    "            hdr_type_grp = file_hdr_grp.create_group(f'{file_hdr_type}')\n",
    "            file_hdr_word_list.append(hdr_type_grp)\n",
    "            if file_hdr_type == 'data_format':\n",
    "                for attr_name in ['daq_major', 'daq_minor', 'data_format_major', 'data_format_minor']:\n",
    "                    if hasattr(parsed_data.file_hdr.data_format, attr_name):\n",
    "                        attr_value = getattr(parsed_data.file_hdr.data_format, attr_name)\n",
    "                        hdr_type_grp.create_dataset(attr_name, data=attr_value)\n",
    "            elif file_hdr_type == \"endian_indicator\":\n",
    "                hdr_type_grp.create_dataset('endian_indicator', data=parsed_data.file_hdr.endian_indicator)\n",
    "        \n",
    "        for det_data_type in parsed_data.detector_hdr:\n",
    "            det_type_grp = detector_hdr_grp.create_group(f'{det_data_type}')\n",
    "            det_hdr_list.append(det_type_grp)\n",
    "            if det_data_type == 'header_number':\n",
    "                det_type_grp.create_dataset('header_number', data=parsed_data.detector_hdr.header_number)\n",
    "            elif det_data_type == 'config_record_len':\n",
    "                det_type_grp.create_dataset('config_record_len', data=parsed_data.detector_hdr.config_record_len)\n",
    "            elif det_data_type == 'repeat_value':\n",
    "                det_type_grp.create_dataset('repeat_value', data=parsed_data.detector_hdr.repeat_value)\n",
    "\n",
    "\n",
    "        # hdrs contains an array of charge and phonon headers\n",
    "        hdrs_grp           = f.create_group('hdrs')\n",
    "        charge_config_grp  = hdrs_grp.create_group('charge_config')\n",
    "        phonon_config_grp  = hdrs_grp.create_group('phonon_config')\n",
    "        hdrs_array         = []\n",
    "        charge_config_list = []\n",
    "        phonon_config_list = []\n",
    "\n",
    "        # Create groups for each header and populate them with relevant datasets\n",
    "        for i, header in enumerate(parsed_data.hdrs):\n",
    "            # Collecting charge_config data\n",
    "            if header.header_number == 0x10002:\n",
    "                # HDF5 groups require unique names if at same level of structure\n",
    "                charge_config_hdr_grp = charge_config_grp.create_group(f'charge_config_{i}')\n",
    "                charge_config_list.append(charge_config_hdr_grp)\n",
    "                hdrs_array.append(charge_config_hdr_grp)\n",
    "                for attr_name in ['charge_config_len', 'detector_code', 'tower_number',\n",
    "                                  'channel_post_amp', 'rtf_offset', 'delta_t', 'trigger_time',\n",
    "                                  'trace_len']:\n",
    "                    if hasattr(header.charge_config, attr_name):\n",
    "                        attr_value = getattr(header.charge_config, attr_name)\n",
    "                        charge_config_hdr_grp.create_dataset(attr_name, data=attr_value)\n",
    "                \n",
    "            # Collecting phonon_config data\n",
    "            elif header.header_number == 0x10001:\n",
    "                phonon_config_hdr_grp = phonon_config_grp.create_group(f'phonon_config_{i}')\n",
    "                phonon_config_list.append(header)\n",
    "                hdrs_array.append(phonon_config_hdr_grp)\n",
    "                for attr_name in ['phonon_config_len', 'detector_code', 'tower_number',\n",
    "                                  'post_amp_gain', 'qet_bias', 'squid_bias', 'squid_lockpoint',\n",
    "                                  'rtf_offset', 'variable_gain', 'delta_t', 'trigger_time', 'trace_len']:\n",
    "                    if hasattr(header.phonon_config, attr_name):\n",
    "                        attr_value = getattr(header.phonon_config, attr_name)\n",
    "                        phonon_config_hdr_grp.create_dataset(attr_name, data=attr_value)\n",
    "        \n",
    "        # Creating groups that can hold each event's records\n",
    "        logical_rcrd_grp       = f.create_group('logical_rcrds')\n",
    "        pulse_grp              = logical_rcrd_grp.create_group('pulse_data')\n",
    "        event_hdr_grp          = logical_rcrd_grp.create_group('event_hdr')\n",
    "        admin_rcrd_grp         = logical_rcrd_grp.create_group('admin_rcrd')\n",
    "        trigger_rcrd_grp       = logical_rcrd_grp.create_group('trigger_rcrd')\n",
    "        tlb_trig_mask_rcrd_grp = logical_rcrd_grp.create_group('tlb_trig_mask_rcrd')\n",
    "        gps_data_grp           = logical_rcrd_grp.create_group('gps_data')\n",
    "        trace_data_grp         = logical_rcrd_grp.create_group('trace_data')\n",
    "        soudan_buffer_grp      = logical_rcrd_grp.create_group('soudan_buffer')\n",
    "\n",
    "        # Initializing arrays to store the created groups of logical_rcrd data\n",
    "        logical_rcrd_array       = []\n",
    "        event_hdr_array          = []\n",
    "        admin_rcrd_array         = []\n",
    "        trigger_rcrd_array       = []\n",
    "        tlb_trig_mask_rcrd_array = []\n",
    "        gps_data_array           = []\n",
    "        trace_data_array         = []\n",
    "        soudan_buffer_array      = []\n",
    "        # Array for storing trace record data\n",
    "        pulse_array              = []\n",
    "\n",
    "        for i, record_option in enumerate(parsed_data.logical_rcrds):\n",
    "            # Storing event_hdr data\n",
    "            events = []\n",
    "            # Loop through attributes of event_hdr and store them in event_group_i\n",
    "            event_group_i = event_hdr_grp.create_group(f'event_group_{i}')\n",
    "            for attr_name in ['event_header_word', 'event_size', 'event_identifier',\n",
    "                            'event_class', 'event_category', 'event_type']:\n",
    "                if hasattr(record_option.event_hdr, attr_name):\n",
    "                    attr_value = getattr(record_option.event_hdr, attr_name)\n",
    "                    event_data = event_group_i.create_dataset(attr_name, data=attr_value)\n",
    "                    events.append(event_data)\n",
    "            event_hdr_array.append(events)\n",
    "\n",
    "            # Storing admin_rcrd data\n",
    "            admins = []\n",
    "            # Loop through attributes of admin_rcrd and store them in admin_group_i\n",
    "            admin_group_i = admin_rcrd_grp.create_group(f'admin_group_{i}')\n",
    "            for attr_name in ['admin_header', 'admin_len', 'series_number_1', 'series_number_2', \n",
    "                              'event_number_in_series', 'seconds_from_epoch', 'time_from_last_event',\n",
    "                              'live_time_from_last_event']:\n",
    "                if hasattr(record_option.admin_rcrd, attr_name):\n",
    "                    attr_value = getattr(record_option.admin_rcrd, attr_name)\n",
    "                    admin_data = admin_group_i.create_dataset(attr_name, attr_value)\n",
    "                    admins.append(admin_data)\n",
    "            admin_rcrd_array.append(admins)\n",
    "\n",
    "            # Storing trigger_rcrd data\n",
    "            triggers = []\n",
    "            # Loop through attributes of trigger_rcrd and store them in trigger_group_i\n",
    "            trigger_group_i = trigger_rcrd_grp.create_group(f'trigger_rcrd_group_{i}')\n",
    "            for attr_name in ['trigger_header', 'trigger_len', 'trigger_time', 'individual_trigger_masks']:\n",
    "                if hasattr(record_option.trigger_rcrd, attr_name):\n",
    "                    attr_value = getattr(record_option.trigger_rcrd, attr_name)\n",
    "                    trigger_data = trigger_group_i.create_dataset(attr_name, data=attr_value)\n",
    "                    triggers.append(trigger_data)\n",
    "            trigger_rcrd_array.append(triggers)\n",
    "\n",
    "            # Storing tlb_trig_mask_rcrd data\n",
    "            tlb_trig_mask = []\n",
    "            # Loop through attributes of tlb_trig_mask_rcrd and store them in tlb_trig_group_i\n",
    "            tlb_trig_group_i = tlb_trig_mask_rcrd_grp.create_group(f'tlb_trig_group_{i}')\n",
    "            for attr_name in ['tlb_mask_header', 'tlb_len', 'tower_mask']:\n",
    "                if hasattr(record_option.tlb_trig_mask_rcrd, attr_name):\n",
    "                    attr_value = getattr(record_option.tlb_trig_mask_rcrd, attr_name)\n",
    "                    tlb_trig_data = tlb_trig_group_i.create_dataset(attr_name, data=attr_value)\n",
    "                    tlb_trig_mask.append(tlb_trig_data)\n",
    "            tlb_trig_mask_rcrd_array.append(tlb_trig_mask)\n",
    "\n",
    "            # Storing gps_data\n",
    "            gps = []\n",
    "            # Loop through attributes of gps_data and store them in gps_data_group_i\n",
    "            gps_data_group_i = gps_data_grp.create_group(f'gps_data_{i}')\n",
    "            # Only tlb_mask_header and length have values if length = 0\n",
    "            if record_option.gps_data.length == 0:\n",
    "                for attr_name in ['tlb_mask_header', 'length']:\n",
    "                    if hasattr(record_option.gps_data, attr_name):\n",
    "                        attr_value = getattr(record_option.gps_data, attr_name)\n",
    "                        gps_dataset = gps_data_group_i.create_dataset(attr_name, data=attr_value)\n",
    "                        gps.append(gps_dataset)\n",
    "            else:\n",
    "                for attr_name in ['tlb_mask_header', 'length', 'gps_year_day', 'gps_status_hour_minute',\n",
    "                                  'gps_microsecs_from_gps_second']:\n",
    "                    if hasattr(record_option.gps_data, attr_name):\n",
    "                        attr_value = getattr(record_option.gps_data, attr_name)\n",
    "                        gps_dataset = gps_data_group_i.create_dataset(attr_name, data=attr_value)\n",
    "                        gps.append(gps_dataset)\n",
    "            gps_data_array.append(gps)\n",
    "\n",
    "            # Loop through attributes of trace_data.trace_rcrds and store them in trace_record_group_i\n",
    "            trace_data_group_i = trace_data_grp.create_group(f'trace_data_group_{i}')\n",
    "            for trace_idx, trace_data in enumerate(record_option.trace_data):\n",
    "                trace = []\n",
    "                # Add sample data to trace\n",
    "                for data in trace_data.sample_data:\n",
    "                    trace.append(data.sample_a)\n",
    "                    trace.append(data.sample_b)\n",
    "\n",
    "                # Collect related trace record information\n",
    "                trace_rcrd = []\n",
    "                trace_record_group_i = trace_data_group_i.create_group(f'trace_record_group_{trace_idx}')\n",
    "                for attr_name in ['trace_header', 'trace_len', 'trace_bookkeeping_header', 'bookkeeping_len',\n",
    "                                  'digitizer_base_address', 'digitizer_channel', 'detector_code', 'timebase_header',\n",
    "                                  'timebase_len', 't0_in_ns', 'delta_t_ns', 'num_of_points', 'second_trace_header',\n",
    "                                  'num_samples']:\n",
    "                    if hasattr(trace_data.trace_rcrds, attr_name):\n",
    "                        attr_value = getattr(trace_data.trace_rcrds, attr_name)\n",
    "                        trace_rcrd_dataset = trace_record_group_i.create_dataset(attr_name, data=attr_value)\n",
    "                        trace_rcrd.append(trace_rcrd_dataset)\n",
    "            pulse_array.append(trace)\n",
    "            trace_data_array.append(trace_rcrd)\n",
    "            print(f'Data samples in event {i}: {len(trace)}')\n",
    "\n",
    "            # Storing soudan_buffer data\n",
    "            soudan_buffer = []\n",
    "            # Loop through attributes of soudan_buffer and store them in soudan_buffer_group_i\n",
    "            soudan_buffer_group_i = soudan_buffer_grp.create_group(f'soudan_buffer_group_{i}')\n",
    "            for attr_name in ['history_buffer_header', 'history_buffer_len', 'num_time_nvt', 'time_nvt',\n",
    "                              'num_veto_mask_words', 'time_n_minus_veto_mask', 'num_trigger_times', \n",
    "                              'trigger_times', 'num_trigger_mask_words', 'trig_times_minus_trig_mask']:\n",
    "                if hasattr(record_option.soudan_buffer, attr_name):\n",
    "                    attr_value = getattr(record_option.soudan_buffer, attr_name)\n",
    "                    soudan_buffer_dataset = soudan_buffer_group_i.create_dataset(attr_name, data=attr_value)\n",
    "                    soudan_buffer.append(soudan_buffer_dataset)\n",
    "            soudan_buffer_array.append(soudan_buffer)\n",
    "        \n",
    "        logical_rcrd_array.append([events, admins, triggers, tlb_trig_mask, gps, soudan_buffer])\n",
    "        pulse_grp.create_dataset('traces', data=pulse_array)\n",
    "        print(f\"Number of events parsed: {len(pulse_array)}\")\n",
    "\n",
    "        \n",
    "\n",
    "input_path  = \"../01120210_0727_F0114\"\n",
    "output_path = \"../parsedh5.hdf5\"\n",
    "# For full parsed file (not using test)\n",
    "#output_path = \"../large_parsedh5.hdf5\"\n",
    "# For final files, save onto novateur network:\n",
    "#output_path = \"/data3/afisher/soudan_output/parsed.txt\"\n",
    "parse_file(input_path, output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
